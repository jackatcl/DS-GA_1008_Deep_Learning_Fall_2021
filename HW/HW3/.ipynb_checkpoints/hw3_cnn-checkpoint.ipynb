{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYszeLBT-8AL"
   },
   "source": [
    "# Homework 3 - Convolutional Neural Nets\n",
    "\n",
    "In this homework, we will be working with google [colab](https://colab.research.google.com/). Google colab allows you to run a jupyter notebook on google servers using a GPU or TPU. To enable GPU support, make sure to press Runtime -> Change Runtime Type -> GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1S52fntBm8ep"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLI0m6U7_lZt"
   },
   "source": [
    "## Cats vs dogs classification\n",
    "\n",
    "To learn about and experiment with convolutional neural nets we will be working on a problem of great importance in computer vision - classifying images of cats and dogs.\n",
    "\n",
    "The problem is so important that there's even an easter egg in colab: go to Tools -> Settings -> Miscellaneous and enable 'Corgi mode' and 'Kitty mode' to get more cats and dogs to classify when you're tired of coding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtsy_bpEC8wp"
   },
   "source": [
    "### Getting the data\n",
    "\n",
    "To get started with the classification, we first need to download and unpack the dataset (note that in jupyter notebooks commands starting with `!` are executed in bash, not in python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "My247KXGJIHe"
   },
   "outputs": [],
   "source": [
    "! wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
    "    -O ./cats_and_dogs_filtered.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Wn0Tw69J9FR"
   },
   "outputs": [],
   "source": [
    "! unzip cats_and_dogs_filtered.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGQA7kgUAHw0"
   },
   "source": [
    "This dataset contains two directories, `train` and `validation`. Both in turn contain two directories with images: `cats` and `dogs`. In `train` we have 1000 images of cats, and another 1000 images of dogs. For `validation`, we have 500 images of each class. Our goal is to implement and train a convolutional neural net to classify these images, i.e. given an image from this dataset, tell if it contains a cat or a dog.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yqS2szPCVRH"
   },
   "outputs": [],
   "source": [
    "! echo 'Training cats examples:' $(find cats_and_dogs_filtered/train/cats -type f | wc -l)\n",
    "! echo 'Training dogs examples:' $(find cats_and_dogs_filtered/train/dogs -type f | wc -l)\n",
    "! echo 'Validation cats examples:' $(find cats_and_dogs_filtered/validation/cats -type f | wc -l)\n",
    "! echo 'Validation dogs examples:' $(find cats_and_dogs_filtered/validation/dogs -type f | wc -l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pr8YteOYC4Da"
   },
   "source": [
    "### Loading the data\n",
    " Now that we have the data on our disk, we need to load it so that we can use it to train our model. In Pytorch ecosystem, we use `Dataset` class, documentation for which can be found [here](https://pytorch.org/docs/stable/data.html). \n",
    "\n",
    " In the case of computer vision, the datasets with the folder structure 'label_name/image_file' are very common, and to process those there's already a class `torchvision.datasets.ImageFolder` (documented [here](https://pytorch.org/vision/0.8/datasets.html)). Torchvision is a Pytorch library with many commonly used tools in computer vision.\n",
    "\n",
    " Another thing we need from Torchvision library is transforms ([documentation](https://pytorch.org/docs/stable/torchvision/transforms.html)). In computer vision, we very often want to transform the images in certain ways. The most common is normalization. Others include flipping, changing saturation, hue, contrast, rotation, and blurring. \n",
    "\n",
    " Below, we create a training, validation and test sets. We use a few transforms for augmentation on the training set, but we don't use anything but resize and normalization for validation and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5nDSr1LLA1s"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image # PIL is a library to process images\n",
    "\n",
    "# These numbers are mean and std values for channels of natural images. \n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Inverse transformation: needed for plotting.\n",
    "unnormalize = transforms.Normalize(\n",
    "   mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "   std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "                                    transforms.Resize((256, 256)),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ColorJitter(hue=.1, saturation=.1, contrast=.1),\n",
    "                                    transforms.RandomRotation(20, resample=Image.BILINEAR),\n",
    "                                    transforms.GaussianBlur(7, sigma=(0.1, 1.0)),\n",
    "                                    transforms.ToTensor(),  # convert PIL to Pytorch Tensor\n",
    "                                    normalize,\n",
    "                                ])\n",
    "\n",
    "validation_transforms = transforms.Compose([\n",
    "                                    transforms.Resize((256, 256)),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    normalize,\n",
    "                                ])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder('./cats_and_dogs_filtered/train', transform=train_transforms)\n",
    "validation_dataset, test_dataset = torch.utils.data.random_split(torchvision.datasets.ImageFolder('./cats_and_dogs_filtered/validation', transform=validation_transforms), [500, 500], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JM4wsBQxFOh-"
   },
   "source": [
    "Let's see what one of the images in the dataset looks like (you can run this cell multiple times to see the effects of different augmentations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvurMzqGLUnX"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 200 # change dpi to make plots bigger\n",
    "\n",
    "def show_normalized_image(img, title=None):\n",
    "  plt.imshow(unnormalize(img).detach().cpu().permute(1, 2, 0))\n",
    "  plt.title(title)\n",
    "  plt.axis('off')\n",
    "\n",
    "show_normalized_image(train_dataset[10][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8MytjMIFl18"
   },
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGPgMBLqFpME"
   },
   "source": [
    "Now is the time to create a model. All models in Pytorch are subclassing `torch.nn.Module`, and have to implement `__init__` and `forward` methods. \n",
    "\n",
    "Below we provide a simple model skeleton, which you need to expand. The places to put your code are marked with `TODO`. Here, we ask you to implement a convolutional neural network containing the following elements:\n",
    "\n",
    "* Convolutional layers (at least two)\n",
    "* Batch Norm\n",
    "* Non-linearity\n",
    "* Pooling layers\n",
    "* A residual connection similar to that of Res-Net\n",
    "* A fully connected layer\n",
    "\n",
    "For some examples of how to implement Pytorch models, please refer to our lab notebooks, such as [this one](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/06-convnet.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mmt1CitJM6xD"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # TODO\n",
    "\n",
    "  def forward(self, x):\n",
    "    # TODO\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEQpspPpHCsE"
   },
   "source": [
    "### Training the model\n",
    "\n",
    "Now we train the model on the dataset. Again, we're providing you with the skeleton with some parts marked as `TODO` to be filled by you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdJT_iYvOTtS"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_loss_and_correct(model, batch, criterion, device):\n",
    "  # Implement forward pass and loss calculation for one batch.\n",
    "  # Remember to move the batch to device.\n",
    "  # \n",
    "  # Return a tuple:\n",
    "  # - loss for the batch (Tensor)\n",
    "  # - number of correctly classified examples in the batch (Tensor)\n",
    "  pass\n",
    "\n",
    "def step(loss, optimizer):\n",
    "  # Implement backward pass and update.\n",
    "  # TODO\n",
    "  pass\n",
    "\n",
    "\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, num_workers=4)\n",
    "model = CNN_Solution()\n",
    "\n",
    "criterion = None # TODO\n",
    "\n",
    "optimizer = None # TODO\n",
    "\n",
    "model.train()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  model = model.cuda()\n",
    "  criterion = criterion.cuda()\n",
    "  device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "validation_losses = []\n",
    "validation_accuracies = []\n",
    "\n",
    "pbar = tqdm(range(N_EPOCHS))\n",
    "\n",
    "for i in pbar:\n",
    "  total_train_loss = 0.0\n",
    "  total_train_correct = 0.0\n",
    "  total_validation_loss = 0.0\n",
    "  total_validation_correct = 0.0\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  for batch in tqdm(train_dataloader, leave=False):\n",
    "    loss, correct = get_loss_and_correct(model, batch, criterion, device)\n",
    "    step(loss, optimizer)\n",
    "    total_train_loss += loss.item()\n",
    "    total_train_correct += correct.item()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch in validation_dataloader:\n",
    "      loss, correct = get_loss_and_correct(model, batch, criterion, device)\n",
    "      total_validation_loss += loss.item()\n",
    "      total_validation_correct += correct.item()\n",
    "\n",
    "  mean_train_loss = total_train_loss / len(train_dataset)\n",
    "  train_accuracy = total_train_correct / len(train_dataset)\n",
    "\n",
    "  mean_validation_loss = total_validation_loss / len(validation_dataset)\n",
    "  validation_accuracy = total_validation_correct / len(validation_dataset)\n",
    "\n",
    "  train_losses.append(mean_train_loss)\n",
    "  validation_losses.append(mean_validation_loss)\n",
    "\n",
    "  train_accuracies.append(train_accuracy)\n",
    "  validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "  pbar.set_postfix({'train_loss': mean_train_loss, 'validation_loss': mean_validation_loss, 'train_accuracy': train_accuracy, 'validation_accuracy': validation_accuracy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAZPa_-HH9S3"
   },
   "source": [
    "Now that the model is trained, we want to visualize the training and validation losses and accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYJtDFiNxCCj"
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(validation_losses, label='validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Losses')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(train_accuracies, label='train')\n",
    "plt.plot(validation_accuracies, label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Accuracies')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucS7X23fJLyH"
   },
   "source": [
    "Now, change your model to achieve at least 75% accuracy on validation set. You can change the model you've implemented, the optimizer, and the augmentations. \n",
    "\n",
    "Looking at the loss and accuracy plots, can you see if your model overfits the trainig set? Why?\n",
    "\n",
    "Answer:\n",
    "`put your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7nIBeacLE42"
   },
   "source": [
    "### Testing the model\n",
    "\n",
    "Now, use the `test_dataset` to get the final accuracy of your model. Visualize some correctly and incorrectly classified examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2J7MqgSyGtT-"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Calculate and show the test_dataset accuracy of your model.\n",
    "# 2. Visualize some correctly and incorrectly classified examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fDrpZ28PT0k"
   },
   "source": [
    "### Visualizing filters\n",
    "\n",
    "In this part, we are going to visualize the output of one of the convolutional layers to see what features they focus on.\n",
    "\n",
    "First, let's get some image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAKNWfnuQNLP"
   },
   "outputs": [],
   "source": [
    "image = validation_dataset[10][0]\n",
    "show_normalized_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hh4uNI22U9dR"
   },
   "source": [
    "Now, we are going to 'clip' our model at different points to get different intermediate representation. \n",
    "Clip your model at two or three different points and plot the filters output.\n",
    "\n",
    "In order to clip the model, you can use `model.children()` method. For example, to get output only after the first 4 layers, you can do:\n",
    "\n",
    "```\n",
    "clipped = nn.Sequential(\n",
    "    *list(model.children()[:4])\n",
    ")\n",
    "intermediate_output = clipped(input)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtTbfc6vQ2P1"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def plot_intermediate_output(result, title):\n",
    "  \"\"\" Plots the intermediate output of shape\n",
    "      N_FILTERS x H x W\n",
    "  \"\"\"\n",
    "  n_filters = result.shape[1]\n",
    "  N = int(math.sqrt(n_filters))\n",
    "  M = (n_filters + N - 1) // N\n",
    "  assert N * M >= n_filters\n",
    "  \n",
    "  fig, axs = plt.subplots(N, M)\n",
    "  fig.suptitle(title)\n",
    "  \n",
    "  for i in range(N):\n",
    "    for j in range(M):\n",
    "      if i*N + j < n_filters:\n",
    "        axs[i][j].imshow(result[0, i*N + j].cpu().detach())\n",
    "        axs[i][j].axis('off')\n",
    "\n",
    "# TODO: \n",
    "# pick a few intermediate representations from your network and plot them using \n",
    "# the provided function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGLDXme0RCIp"
   },
   "source": [
    "What can you say about those filters? What features are they focusing on?\n",
    "\n",
    "Anwer: `Your answer here`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": " hw3_cnn",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
